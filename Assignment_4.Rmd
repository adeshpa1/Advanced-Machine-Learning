---
title: "Assignment_4"
author: "Amruta Deshpande"
output: html_document
---

# Problem:

Use any or all of the methods to improve weather forecasting for the problem we discussed in class. For example, by adjusting the number of units in each recurrent layer in the stacked setup, or using layer_lstm() instead of layer_gru().

You can also try experimenting with a combination of 1d_convnets and rnn.

Donâ€™t forget to eventually run the best-performing models (in terms of validation MAE) on the test set! Best three performers over baseline performance of 0.29 get extra points. And include checkpoints and tensor board commands. 

# Solution :


# Defining Libraries and redaing the data: 

```{r}
 
library(tibble)
library(readr)
library(keras)
library(tensorflow)
data <- read_csv("./jena_climate_2009_2016.csv")

```

# Normalizing the data:

```{r}

train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)

```

# defining generator function for RNN Model:

```{r}

generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}

lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

```

# Building a RNN model and defining training Model:

```{r}
Train_Gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)
```

# Validation Model:

```{r}
Valid_Gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)
```

# Testing Model

```{r}
Test_Gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
```

# validation & Testing for RNN:

```{r}
val_steps <- (300000 - 200001 - lookback) / batch_size
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

# Model:

```{r}
model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_lstm(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)
```

# Compiling the model:

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = c("mse")
)
```

# **Inspecting and Monitoring Model**

# CallBack:

```{r}
callbacks_list1 <- list(
  callback_early_stopping(
	  monitor = "acc",
	  patience = 1),
  callback_model_checkpoint(
	filepath = "Callback_model.h5",
	monitor = "val_loss",
	save_best_only = TRUE
  )
)
```

# Reduce Learining Rate:

```{r}
callbacks_list2 <- list(
  callback_reduce_lr_on_plateau(
	  monitor = "acc",
	  patience = 1),
  callback_model_checkpoint(
	filepath = "ReducedLearining_model.h5",
	monitor = "val_loss",
	save_best_only = TRUE
  )
)
```

# Tensorboard:

```{r}
tensorboard("Tensorboard_log_dir")

callbacks_list3 <- list(
  callback_tensorboard(
	log_dir = "Tensorboard_log_dir",
	histogram_freq = 1,
	embeddings_freq = 1,
  )
)
```

## Model implementation on training generator and validating on Validation generator using the different methods of inspecting the model.

```{r}
history3 <- model %>% fit_generator(
  Train_Gen,
  steps_per_epoch = 500,
  epochs = 40,
  callbacks = callbacks_list3,
  validation_data = Valid_Gen,
  validation_steps = val_steps)
```

# Plotting:

```{r}
plot(history3)
```

### Depending upon the best hypertuned parameters we get from the model evaluation on validation datasetr the same epochs values or the callback method the test data will be exectuted to get the output.
