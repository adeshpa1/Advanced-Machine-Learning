---
title: "Assignment_3"
author: "Amruta Deshpande"
output: html_document
---

# Problem:

Use any or all of the methods to improve weather forecasting for the problem we discussed in class. For example, by adjusting the number of units in each recurrent layer in the stacked setup, or using layer_lstm() instead of layer_gru().

You can also try experimenting with a combination of 1d_convnets and rnn.

Donâ€™t forget to eventually run the best-performing models (in terms of validation MAE) on the test set! 

Best three performers over baseline performance of 0.29 get extra points.

# Solution:

# Defining Libraries and redaing the data: 

```{r}
 
library(tibble)
library(readr)
library(keras)
library(tensorflow)
data <- read_csv("./jena_climate_2009_2016.csv")

```

# Normalizing the data:

```{r}

train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)

```

# defining generator function for RNN Model:

```{r}

generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}

lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

```

# Building a RNN model and defining training Model:

```{r}
Train_Gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)
```

# Validation Model:

```{r, eval=FALSE}
Valid_Gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)
```

# Testing Model

```{r}
Test_Gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
```

# validation & Testing for RNN:


```{r}
val_steps <- (300000 - 200001 - lookback) / batch_size
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

# Model:

```{r}
model <- keras_model_sequential() %>% 
  layer_lstm(units = 32, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_lstm(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)
```

# Compiling the model:

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = c("mse")
)
```

# Model implementation:

```{r}
history <- model %>% fit_generator(
  Train_Gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = Valid_Gen,
  validation_steps = val_steps)
```

# plotting:

```{r}
plot(history)
```

# Model Implementation on Test data:

```{r}
model %>% fit_generator(
  Train_Gen,
  steps_per_epoch = 500,
  epochs = 15,
  validation_data = Test_Gen,
  validation_steps = test_steps)
```
